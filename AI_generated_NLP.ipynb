{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_generated_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dWgxcQQ2BpqXYrFd5bNE_uW684UVyVyn",
      "authorship_tag": "ABX9TyO8rW/pJax931uXINA2594p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spe301/AI-generated-AI/blob/main/AI_generated_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkPOituvoRXT",
        "outputId": "b070e150-318f-46f5-9591-2766f85803a1"
      },
      "source": [
        "!pip install Potosnail==0.2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Potosnail==0.2.1 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: xgboost<=1.3.1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.90)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.22.2.post1)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (2.4.1)\n",
            "Requirement already satisfied: beautifulsoup4==4.9.3 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (4.9.3)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.19.5)\n",
            "Requirement already satisfied: statsmodels>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.12.2)\n",
            "Requirement already satisfied: urllib3==1.25.11 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.25.11)\n",
            "Requirement already satisfied: lxml==4.6.1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (4.6.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.11.1)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (3.3.4)\n",
            "Requirement already satisfied: regex==2020.10.15 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (2020.10.15)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost<=1.3.1->Potosnail==0.2.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1->Potosnail==0.2.1) (1.0.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.12)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.3.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.10.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.1.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.12.4)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.4.1)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.32.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.10.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.12.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4==4.9.3->Potosnail==0.2.1) (2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.3->Potosnail==0.2.1) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.3->Potosnail==0.2.1) (2018.9)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.0->Potosnail==0.2.1) (0.5.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (2.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (7.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (0.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow>=2.4.0->Potosnail==0.2.1) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apVtNy1KLP64"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from potosnail import *\r\n",
        "\r\n",
        "ml = MachineLearning()\r\n",
        "dl = DeepLearning()\r\n",
        "dh = DataHelper()\r\n",
        "ev = Evaluater()\r\n",
        "al = Algorithms()\r\n",
        "wr = Wrappers()\r\n",
        "st = Stats()\r\n",
        "\r\n",
        "def SuperBear(df, gridding=False):\r\n",
        "  dh = DataHelper()\r\n",
        "  '''gets the nlp results dataset ready for modeling'''\r\n",
        "  df['regularizer'] = df['regularizer'].fillna('None')\r\n",
        "  df['stacking'] = df['stacking'].astype(int)\r\n",
        "  df['dropout'] = df['dropout'].astype(int)\r\n",
        "  df['bidirectional'] = df['bidirectional'].astype(int)\r\n",
        "  act = dh.OHE(df['activation'])\r\n",
        "  reg = dh.OHE(df['regularizer'])\r\n",
        "  opt = dh.OHE(df['optimizer'])\r\n",
        "  method = dh.OHE(df['method'])\r\n",
        "  df = df.drop(['activation', 'regularizer', 'optimizer', 'method'], axis='columns')\r\n",
        "  df = pd.concat([df, act, reg, opt, method], axis='columns')\r\n",
        "  if gridding == True:\r\n",
        "    return df\r\n",
        "  df['val_loss'] = df['val_loss'].fillna(max(df['val_loss']))\r\n",
        "  df['loss'] = df['loss'].fillna(max(df['loss']))\r\n",
        "  return df"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298ew3nYB7sw"
      },
      "source": [
        "df1 = pd.read_csv('https://raw.githubusercontent.com/spe301/AI-generated-AI/main/Data/NLP8.csv').drop(['Unnamed: 0'], axis='columns')\r\n",
        "nlp = SuperBear(df1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZDLpl8gOgPw"
      },
      "source": [
        "# EDA Questions\r\n",
        "\r\n",
        "1. is the difference in validation loss between 9,000 point datasets and 360 point datasets statistically signifigant?\r\n",
        "2. is the difference in 'difference of validation loss and loss' between dropout and no dropout statistically signifigant?\r\n",
        "3. Which model had the highest epochs and how many standard deviations away were it's accuracies from average (z score)\r\n",
        "4. How is n_nodes correlated for smaller datasets compared to larger datasets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUHZv0CGOckf",
        "outputId": "6716e201-24bc-467f-d4f3-49036361313c"
      },
      "source": [
        "from scipy.stats import ttest_ind\r\n",
        "\r\n",
        "s1 = nlp.loc[nlp['len_dataset'] == 360]['val_loss'].sample(100)\r\n",
        "s2 = nlp.loc[nlp['len_dataset'] != 360]['val_loss'].sample(100)\r\n",
        "ttest_ind(s1, s2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_indResult(statistic=8.7231388195817, pvalue=1.106088544388026e-15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_iO6nEuXK1G"
      },
      "source": [
        "validation loss is signifigantly lower with larger dataset\r\n",
        "\r\n",
        "conclusion: Deep NLP works better for larger datasets with thousands of datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlQcrC7vTvSG",
        "outputId": "5b8a45e0-263f-4696-a256-72e6406c7ea6"
      },
      "source": [
        "ldd = nlp.loc[nlp['dropout'] == 1]['val_loss'] - nlp.loc[nlp['dropout'] == 1]['loss']\r\n",
        "s1 = ldd.sample(100)\r\n",
        "ldno = nlp.loc[nlp['dropout'] == 0]['val_loss'] - nlp.loc[nlp['dropout'] == 0]['loss']\r\n",
        "s2 = ldno.sample(100)\r\n",
        "ttest_ind(s1, s2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_indResult(statistic=-0.09511000937926599, pvalue=0.9243236333863147)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZRbMuVhbER2"
      },
      "source": [
        "We can accept the null hypothesis that dropout does not cause a signifigant change in loss\r\n",
        "\r\n",
        "conclusion: adding a dropout layer is unlikley to decrease overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6GXfgrOdu8o",
        "outputId": "3d360010-fdd3-40ef-c58a-9017371c62f9"
      },
      "source": [
        "acc50 = np.mean(list(nlp.loc[nlp['epochs'] == max(nlp['epochs'])]['accuracy']))\r\n",
        "val50 = np.mean(list(nlp.loc[nlp['epochs'] == max(nlp['epochs'])]['val_accuracy']))\r\n",
        "\r\n",
        "st.Z(nlp['accuracy'], acc50), st.Z(nlp['val_accuracy'], val50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8442059083699316, -0.43293229648074893)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywG2OgCeDLH",
        "outputId": "1c3fced9-3698-4867-a308-17b4b3bcd5a3"
      },
      "source": [
        "val50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4987113395917047"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHjK9_Md9ZZ"
      },
      "source": [
        "Models that were trained for 50 epochs had lower accuracies than average but were also less overfit.\r\n",
        "\r\n",
        "Conclusion: overfitting seems to be a major problem, setting epochs higher while figuring out how to increase accuracy may be well worth it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0b51TUxe_Ai",
        "outputId": "2539c24a-a6c2-4741-a6c4-baccaca08952"
      },
      "source": [
        "np.mean(nlp['epochs'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.6425"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "QxqJbdJTfF4G",
        "outputId": "909c7627-db42-4246-b32f-d0957eb76b0c"
      },
      "source": [
        "kpi_list = ['accuracy', 'loss', 'val_accuracy', 'val_loss']\r\n",
        "kpi = nlp[kpi_list]\r\n",
        "\r\n",
        "scores = []\r\n",
        "for i in range(len(nlp)):\r\n",
        "  ts = (1 - (kpi['loss'][i] / max(kpi['loss'])) + kpi['accuracy'][i])/2\r\n",
        "  vs = (1 - (kpi['val_loss'][i] / max(kpi['val_loss'])) + kpi['val_accuracy'][i])/2\r\n",
        "  score = (ts+vs) - abs(ts-vs)\r\n",
        "  scores.append(score)\r\n",
        "\r\n",
        "nlp2 = nlp.drop(kpi_list, axis='columns')\r\n",
        "nlp2['quality'] = scores\r\n",
        "nlp2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_dim</th>\n",
              "      <th>embedding</th>\n",
              "      <th>nodes</th>\n",
              "      <th>stacking</th>\n",
              "      <th>dropout</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>epochs</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>n_features</th>\n",
              "      <th>dominant_class</th>\n",
              "      <th>relu</th>\n",
              "      <th>tanh</th>\n",
              "      <th>L1</th>\n",
              "      <th>L2</th>\n",
              "      <th>None</th>\n",
              "      <th>adam</th>\n",
              "      <th>rmsprop</th>\n",
              "      <th>sgd</th>\n",
              "      <th>GRU</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.338706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.361174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.361862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.387474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>1724</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.405428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.395079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.341054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.342763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.402762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      output_dim  embedding  nodes  stacking  ...  sgd  GRU  LSTM   quality\n",
              "0              3      12300     64         0  ...  0.0  1.0   0.0  1.338706\n",
              "1              3      12300     64         1  ...  0.0  1.0   0.0  1.361174\n",
              "2              3      12300     64         0  ...  0.0  1.0   0.0  1.361862\n",
              "3              3      12300     64         1  ...  0.0  1.0   0.0  1.387474\n",
              "4              2      45589     64         0  ...  0.0  1.0   0.0  0.490305\n",
              "...          ...        ...    ...       ...  ...  ...  ...   ...       ...\n",
              "1995           3      12300    256         0  ...  0.0  0.0   1.0  1.405428\n",
              "1996           3      12300    256         1  ...  0.0  0.0   1.0  1.395079\n",
              "1997           3      12300    256         1  ...  0.0  0.0   1.0  1.341054\n",
              "1998           3      12300    256         1  ...  0.0  0.0   1.0  1.342763\n",
              "1999           3      12300    256         1  ...  0.0  0.0   1.0  1.402762\n",
              "\n",
              "[2000 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtcZVDg-DcZ"
      },
      "source": [
        "counts = list(np.unique(nlp2['nodes']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "wS68-1RV9vlJ",
        "outputId": "2d3ca7f7-1694-4429-97b5-f9b33b75a21b"
      },
      "source": [
        "import seaborn as sns\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "smaller = nlp2.loc[nlp2['len_dataset']==360]\r\n",
        "bigger = nlp2.loc[nlp2['len_dataset']!=360]\r\n",
        "\r\n",
        "quals1 = []\r\n",
        "quals2 = []\r\n",
        "for count in counts:\r\n",
        "  avg_score1 = np.mean(smaller.loc[smaller['nodes'] == count]['quality'])\r\n",
        "  avg_score2 = np.mean(bigger.loc[bigger['nodes'] == count]['quality'])\r\n",
        "  quals1.append(avg_score1)\r\n",
        "  quals2.append(avg_score2)\r\n",
        "\r\n",
        "sns.barplot(counts, quals1)\r\n",
        "plt.title('small datasets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'small datasets')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "_w4ErkX_BVgk",
        "outputId": "f2d36042-ecf7-4232-f048-dfcedc59922a"
      },
      "source": [
        "sns.barplot(counts, quals2)\r\n",
        "plt.title('large datasets')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'large datasets')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9wO1pmspmrW"
      },
      "source": [
        "train, val = dh.HoldOut(nlp2)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nussxRqSpy8U",
        "outputId": "cd6f7b22-9221-4f8e-950d-75edeab8ab14"
      },
      "source": [
        "kit = wr.WrapML(train, 'quality', 'regression', quiet=False)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86.71% accuracy, untuned model, raw data\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    1.9s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "84.34% accuracy, tuned model, raw data\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    1.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "86.78% accuracy, tuned model, data is scaled with minmax scaler\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "42.33% accuracy, tuned model, features have been reduced to 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    1.5s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giZ2Wocj1LLs"
      },
      "source": [
        "model = list(kit.values())[0][0]\r\n",
        "X = list(kit.values())[0][1]\r\n",
        "y = list(kit.values())[0][2]\r\n",
        "Xval = val.drop(['quality'], axis='columns')\r\n",
        "yval = val['quality']"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjlE5wXx1M9P",
        "outputId": "7e17ab8e-d423-45e5-c63d-ebc5af60d2ae"
      },
      "source": [
        "model"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "q9C3qQqk2CA9",
        "outputId": "8f47379c-ff2d-4ac9-8f6b-0f85af7c97d8"
      },
      "source": [
        "'''grid = {'max_depth': [11, 12, 13], 'max_leaf_nodes': [9, 12, 15], 'min_samples_leaf': [3, 4]}\r\n",
        "\r\n",
        "clf = ml.Optimize(model, grid, X, y)\r\n",
        "clf'''"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"grid = {'max_depth': [11, 12, 13], 'max_leaf_nodes': [9, 12, 15], 'min_samples_leaf': [3, 4]}\\n\\nclf = ml.Optimize(model, grid, X, y)\\nclf\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itXCCOyP2RIl",
        "outputId": "c0f42a15-1679-4929-859a-bc3c71bc4a6d"
      },
      "source": [
        "ev.EvaluateRegressor(model, X, Xval, y, yval)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     predicted    actual     error     %error\n",
              " 0     1.487445  1.487364  0.000082   0.999945\n",
              " 1     1.069263  1.103579  0.034316   3.109541\n",
              " 2     1.156446  1.315980  0.159535  12.122864\n",
              " 3     1.326643  1.380297  0.053654   3.887137\n",
              " 4     1.410629  1.409924  0.000705   0.999500\n",
              " ..         ...       ...       ...        ...\n",
              " 195   1.260441  1.376686  0.116245   8.443810\n",
              " 196   1.361358  1.350602  0.010756   0.992099\n",
              " 197   1.487391  1.487356  0.000035   0.999977\n",
              " 198   1.165965  1.150504  0.015461   0.986740\n",
              " 199   1.429547  1.439513  0.009965   0.692257\n",
              " \n",
              " [200 rows x 4 columns], 0.006810066355932055, 80.08)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mbd3qa44gz"
      },
      "source": [
        "X = np.array(X)\r\n",
        "Xval = np.array(Xval)\r\n",
        "y = np.array(y)\r\n",
        "yval = np.array(yval)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzF--3Yx5Knd",
        "outputId": "c21071de-8415-4739-e235-043046b220dd"
      },
      "source": [
        "dm = dl.FastNN('regression', 'mse', output_dim=1)\r\n",
        "history = dm.fit(X, y, epochs=150, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "41/41 [==============================] - 1s 6ms/step - loss: 484083.4777 - val_loss: 1991.7677\n",
            "Epoch 2/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2191.6838 - val_loss: 89.5290\n",
            "Epoch 3/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 42.3429 - val_loss: 4.7632\n",
            "Epoch 4/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 4.8401 - val_loss: 3.1729\n",
            "Epoch 5/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.8938 - val_loss: 3.1795\n",
            "Epoch 6/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 4.1031 - val_loss: 2.9666\n",
            "Epoch 7/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 3.7087 - val_loss: 2.9988\n",
            "Epoch 8/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.6771 - val_loss: 2.7946\n",
            "Epoch 9/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 3.4523 - val_loss: 2.7864\n",
            "Epoch 10/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 3.0312 - val_loss: 2.6222\n",
            "Epoch 11/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.1370 - val_loss: 2.5405\n",
            "Epoch 12/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.1574 - val_loss: 2.5392\n",
            "Epoch 13/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.3164 - val_loss: 2.6782\n",
            "Epoch 14/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2.8354 - val_loss: 2.3995\n",
            "Epoch 15/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 3.1402 - val_loss: 2.1499\n",
            "Epoch 16/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2.6571 - val_loss: 2.0650\n",
            "Epoch 17/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2.6612 - val_loss: 1.9078\n",
            "Epoch 18/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2.4044 - val_loss: 1.8175\n",
            "Epoch 19/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 2.3591 - val_loss: 1.7714\n",
            "Epoch 20/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.0674 - val_loss: 1.6101\n",
            "Epoch 21/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.1118 - val_loss: 1.4828\n",
            "Epoch 22/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.8891 - val_loss: 1.7148\n",
            "Epoch 23/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.7450 - val_loss: 1.3192\n",
            "Epoch 24/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.8069 - val_loss: 1.2428\n",
            "Epoch 25/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.6659 - val_loss: 1.2468\n",
            "Epoch 26/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.5164 - val_loss: 1.1179\n",
            "Epoch 27/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.3034 - val_loss: 1.0362\n",
            "Epoch 28/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.3657 - val_loss: 1.0392\n",
            "Epoch 29/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.0901 - val_loss: 0.9904\n",
            "Epoch 30/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 1.1329\n",
            "Epoch 31/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.0947 - val_loss: 1.0563\n",
            "Epoch 32/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.0226 - val_loss: 0.8419\n",
            "Epoch 33/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7956 - val_loss: 0.7014\n",
            "Epoch 34/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8758 - val_loss: 0.6379\n",
            "Epoch 35/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.9133 - val_loss: 0.6080\n",
            "Epoch 36/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6516 - val_loss: 0.5138\n",
            "Epoch 37/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6310 - val_loss: 0.4856\n",
            "Epoch 38/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.4521\n",
            "Epoch 39/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4688 - val_loss: 0.4857\n",
            "Epoch 40/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4769 - val_loss: 0.3798\n",
            "Epoch 41/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5328 - val_loss: 0.4133\n",
            "Epoch 42/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4410 - val_loss: 0.3547\n",
            "Epoch 43/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4210 - val_loss: 0.3160\n",
            "Epoch 44/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4125 - val_loss: 0.3244\n",
            "Epoch 45/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3634 - val_loss: 0.2549\n",
            "Epoch 46/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2947 - val_loss: 0.2385\n",
            "Epoch 47/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2881 - val_loss: 0.2685\n",
            "Epoch 48/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3051 - val_loss: 0.2592\n",
            "Epoch 49/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2901 - val_loss: 0.1956\n",
            "Epoch 50/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2387 - val_loss: 0.1911\n",
            "Epoch 51/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2446 - val_loss: 0.2547\n",
            "Epoch 52/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2494 - val_loss: 0.1726\n",
            "Epoch 53/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2022 - val_loss: 0.3105\n",
            "Epoch 54/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2543 - val_loss: 0.1758\n",
            "Epoch 55/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2078 - val_loss: 0.2337\n",
            "Epoch 56/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2146 - val_loss: 0.3399\n",
            "Epoch 57/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2147 - val_loss: 0.1378\n",
            "Epoch 58/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1998 - val_loss: 0.1280\n",
            "Epoch 59/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2121 - val_loss: 0.1286\n",
            "Epoch 60/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1678 - val_loss: 0.2212\n",
            "Epoch 61/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1703 - val_loss: 0.1289\n",
            "Epoch 62/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2100 - val_loss: 0.1301\n",
            "Epoch 63/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1590 - val_loss: 0.1480\n",
            "Epoch 64/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1856 - val_loss: 0.2332\n",
            "Epoch 65/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 0.1195\n",
            "Epoch 66/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1544 - val_loss: 0.1424\n",
            "Epoch 67/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1621 - val_loss: 0.1639\n",
            "Epoch 68/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1635 - val_loss: 0.1659\n",
            "Epoch 69/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1435 - val_loss: 0.1503\n",
            "Epoch 70/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1477 - val_loss: 0.0968\n",
            "Epoch 71/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1782 - val_loss: 0.1356\n",
            "Epoch 72/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1793 - val_loss: 0.1850\n",
            "Epoch 73/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2458 - val_loss: 0.2003\n",
            "Epoch 74/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1451 - val_loss: 0.1129\n",
            "Epoch 75/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2370 - val_loss: 0.1301\n",
            "Epoch 76/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1639 - val_loss: 0.1517\n",
            "Epoch 77/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1216 - val_loss: 0.0946\n",
            "Epoch 78/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1319 - val_loss: 0.1999\n",
            "Epoch 79/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1653 - val_loss: 0.1516\n",
            "Epoch 80/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2010 - val_loss: 0.1023\n",
            "Epoch 81/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1157 - val_loss: 0.2386\n",
            "Epoch 82/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2013 - val_loss: 0.0919\n",
            "Epoch 83/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1392 - val_loss: 0.0995\n",
            "Epoch 84/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1453 - val_loss: 0.6743\n",
            "Epoch 85/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3437 - val_loss: 0.1297\n",
            "Epoch 86/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1747 - val_loss: 0.7228\n",
            "Epoch 87/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6514 - val_loss: 0.2381\n",
            "Epoch 88/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2357 - val_loss: 0.1363\n",
            "Epoch 89/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1900 - val_loss: 0.5549\n",
            "Epoch 90/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2624 - val_loss: 0.9052\n",
            "Epoch 91/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4854 - val_loss: 0.0759\n",
            "Epoch 92/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1057 - val_loss: 0.1211\n",
            "Epoch 93/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1624 - val_loss: 0.1508\n",
            "Epoch 94/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1184 - val_loss: 0.0650\n",
            "Epoch 95/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1188 - val_loss: 0.0837\n",
            "Epoch 96/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1376 - val_loss: 0.0715\n",
            "Epoch 97/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1386 - val_loss: 0.1361\n",
            "Epoch 98/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1411 - val_loss: 0.0796\n",
            "Epoch 99/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1292 - val_loss: 0.0884\n",
            "Epoch 100/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3489 - val_loss: 2.1607\n",
            "Epoch 101/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.4326 - val_loss: 0.1230\n",
            "Epoch 102/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2351 - val_loss: 0.2268\n",
            "Epoch 103/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2736 - val_loss: 16.4027\n",
            "Epoch 104/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.9366 - val_loss: 2.9348\n",
            "Epoch 105/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 4.5805 - val_loss: 2.9216\n",
            "Epoch 106/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 6.8880 - val_loss: 0.9417\n",
            "Epoch 107/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 7.5738 - val_loss: 8.3739\n",
            "Epoch 108/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.3628 - val_loss: 0.8527\n",
            "Epoch 109/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.5730 - val_loss: 0.0839\n",
            "Epoch 110/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3695 - val_loss: 0.1597\n",
            "Epoch 111/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1373 - val_loss: 0.9441\n",
            "Epoch 112/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 743.3554 - val_loss: 1043.1096\n",
            "Epoch 113/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1516.4992 - val_loss: 339.8692\n",
            "Epoch 114/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 485.2713 - val_loss: 12.5482\n",
            "Epoch 115/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 123.9261 - val_loss: 46.8160\n",
            "Epoch 116/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 15.3445 - val_loss: 2.3981\n",
            "Epoch 117/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8144 - val_loss: 0.3140\n",
            "Epoch 118/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2097 - val_loss: 1.2465\n",
            "Epoch 119/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7284 - val_loss: 0.1911\n",
            "Epoch 120/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1769 - val_loss: 0.4502\n",
            "Epoch 121/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2007 - val_loss: 0.1268\n",
            "Epoch 122/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2245 - val_loss: 0.0937\n",
            "Epoch 123/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2481 - val_loss: 0.0889\n",
            "Epoch 124/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1505 - val_loss: 0.1424\n",
            "Epoch 125/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1055 - val_loss: 0.3010\n",
            "Epoch 126/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2302 - val_loss: 0.1070\n",
            "Epoch 127/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4750 - val_loss: 1.3641\n",
            "Epoch 128/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.4871 - val_loss: 1.9277\n",
            "Epoch 129/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.8637 - val_loss: 2.8648\n",
            "Epoch 130/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.7029 - val_loss: 27.9629\n",
            "Epoch 131/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 11.5377 - val_loss: 31.3174\n",
            "Epoch 132/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 45.6037 - val_loss: 2.7917\n",
            "Epoch 133/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 14.1199 - val_loss: 0.3285\n",
            "Epoch 134/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6918 - val_loss: 0.1278\n",
            "Epoch 135/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3516 - val_loss: 1.4814\n",
            "Epoch 136/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3346 - val_loss: 0.1250\n",
            "Epoch 137/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1146 - val_loss: 0.2741\n",
            "Epoch 138/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.9234 - val_loss: 48.9872\n",
            "Epoch 139/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 46.2079 - val_loss: 9.0457\n",
            "Epoch 140/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 63.4571 - val_loss: 26.8032\n",
            "Epoch 141/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 25.1862 - val_loss: 14.2509\n",
            "Epoch 142/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.7234 - val_loss: 0.0818\n",
            "Epoch 143/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 23.2046 - val_loss: 19.9190\n",
            "Epoch 144/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 10.1303 - val_loss: 30.5440\n",
            "Epoch 145/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 43.3252 - val_loss: 112.4694\n",
            "Epoch 146/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 155.7431 - val_loss: 666.9795\n",
            "Epoch 147/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 306.8056 - val_loss: 5.0278\n",
            "Epoch 148/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 34.9515 - val_loss: 121.3666\n",
            "Epoch 149/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 43.0370 - val_loss: 1.1106\n",
            "Epoch 150/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.0280 - val_loss: 0.5234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pkE1KbW55Nj",
        "outputId": "4c4d53c0-da52-42e5-aad7-731c999d455a"
      },
      "source": [
        "dm.evaluate(Xval, yval)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 0s 1ms/step - loss: 0.5770\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5769760012626648"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "hmZf6qwR6Fsa",
        "outputId": "9c7ef4af-b817-4f2f-a2a9-39aa68345a0c"
      },
      "source": [
        "ev.ViewLoss(history)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "nx5SkL8PB-WR",
        "outputId": "e278ed89-50f1-41ae-f780-1ea8c2428e6e"
      },
      "source": [
        "def BunnyPinkears(params, len_dataset, n_features, dominant_class):\r\n",
        "  '''puts all possible gridsearch combinations in a dataframe'''\r\n",
        "  n = list(params.keys())\r\n",
        "  lst1 = []\r\n",
        "  lst2 = []\r\n",
        "  lst3 = []\r\n",
        "  lst4 = []\r\n",
        "  lst5 = []\r\n",
        "  lst6 = []\r\n",
        "  lst7 = []\r\n",
        "  lst8 = []\r\n",
        "  lst9 = []\r\n",
        "  lst10 = []\r\n",
        "  lst11 = []\r\n",
        "  lst12 = []\r\n",
        "  for i in range(len(params[n[0]])):\r\n",
        "    var1 = params[n[0]][i]\r\n",
        "    for i in range(len(params[n[1]])):\r\n",
        "      var2 = params[n[1]][i]\r\n",
        "      for i in range(len(params[n[2]])):\r\n",
        "        var3 = params[n[2]][i]\r\n",
        "        for i in range(len(params[n[3]])):\r\n",
        "          var4 = params[n[3]][i]\r\n",
        "          for i in range(len(params[n[4]])):\r\n",
        "            var5 = params[n[4]][i]\r\n",
        "            for i in range(len(params[n[5]])):\r\n",
        "              var6 = params[n[5]][i]\r\n",
        "              for i in range(len(params[n[6]])):\r\n",
        "                var7 = params[n[6]][i]\r\n",
        "                for i in range(len(params[n[7]])):\r\n",
        "                  var8 = params[n[7]][i]\r\n",
        "                  for i in range(len(params[n[8]])):\r\n",
        "                    var9 = params[n[8]][i]\r\n",
        "                    for i in range(len(params[n[9]])):\r\n",
        "                      var10 = params[n[9]][i]\r\n",
        "                      for i in range(len(params[n[10]])):\r\n",
        "                        var11 = params[n[10]][i]\r\n",
        "                        for i in range(len(params[n[11]])):\r\n",
        "                          var12 = params[n[11]][i]\r\n",
        "                          lst1.append(var1)\r\n",
        "                          lst2.append(var2)\r\n",
        "                          lst3.append(var3)\r\n",
        "                          lst4.append(var4)\r\n",
        "                          lst5.append(var5)\r\n",
        "                          lst6.append(var6)\r\n",
        "                          lst7.append(var7)\r\n",
        "                          lst8.append(var8)\r\n",
        "                          lst9.append(var9)\r\n",
        "                          lst10.append(var10)\r\n",
        "                          lst11.append(var11)\r\n",
        "                          lst12.append(var12)\r\n",
        "  df = pd.DataFrame(lst1)\r\n",
        "  df.columns = [n[0]]\r\n",
        "  df[n[1]] = lst2\r\n",
        "  df[n[2]] = lst3\r\n",
        "  df[n[3]] = lst4\r\n",
        "  df[n[4]] = lst5\r\n",
        "  df[n[5]] = lst6\r\n",
        "  df[n[6]] = lst7\r\n",
        "  df[n[7]] = lst8\r\n",
        "  df[n[8]] = lst9\r\n",
        "  df[n[9]] = lst10\r\n",
        "  df[n[10]] = lst11\r\n",
        "  df[n[11]] = lst12\r\n",
        "  df['len_dataset'] = [len_dataset] * len(df)\r\n",
        "  df['n_features'] = [n_features] * len(df)\r\n",
        "  df['dominant_class'] = [dominant_class] * len(df)\r\n",
        "  return df\r\n",
        "                    \r\n",
        "                    \r\n",
        "grid = {'output_dim': [2], #because we have 2 classes\r\n",
        "          'embedding': [45589], #vocab is number of unique words in dataset\r\n",
        "          'nodes': list(range(32, 68, 4)), #we will test between 32 and 64 nodes for the first layer\r\n",
        "          'activation': ['tanh', 'relu'], #we will test between relu and tanh for activation function\r\n",
        "          'regularizer': ['L1', None, 'L2'], #we will use L1 reqularization to prevent overfitting\r\n",
        "          'stacking': [False, True], #stacking makes the first 2 layers the same, we will not do this\r\n",
        "          'dropout': [False, True], #we will not use dropout because we are already using L1 regularization\r\n",
        "          'optimizer': ['adam', 'rmsprop', 'sgd'], #we will test between adam and rmsprop for optimization function\r\n",
        "          'method': ['LSTM', 'GRU'], #we will test between using an LSTM cell and a GRU cell\r\n",
        "          'bidirectional': [True, False], 'epochs': list(range(5, 60, 5)), 'batch_size': [32, 64]}\r\n",
        "\r\n",
        "InpGrid = BunnyPinkears(grid, 360, 17, 0.5)    \r\n",
        "NlpGrid = SuperBear(InpGrid, gridding=True)\r\n",
        "NlpGrid"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_dim</th>\n",
              "      <th>embedding</th>\n",
              "      <th>nodes</th>\n",
              "      <th>stacking</th>\n",
              "      <th>dropout</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>epochs</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>n_features</th>\n",
              "      <th>dominant_class</th>\n",
              "      <th>relu</th>\n",
              "      <th>tanh</th>\n",
              "      <th>L1</th>\n",
              "      <th>L2</th>\n",
              "      <th>None</th>\n",
              "      <th>adam</th>\n",
              "      <th>rmsprop</th>\n",
              "      <th>sgd</th>\n",
              "      <th>GRU</th>\n",
              "      <th>LSTM</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57019</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57020</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57021</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57022</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57023</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57024 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       output_dim  embedding  nodes  stacking  ...  rmsprop  sgd  GRU  LSTM\n",
              "0               2      45589     32         0  ...      0.0  0.0  0.0   1.0\n",
              "1               2      45589     32         0  ...      0.0  0.0  0.0   1.0\n",
              "2               2      45589     32         0  ...      0.0  0.0  0.0   1.0\n",
              "3               2      45589     32         0  ...      0.0  0.0  0.0   1.0\n",
              "4               2      45589     32         0  ...      0.0  0.0  0.0   1.0\n",
              "...           ...        ...    ...       ...  ...      ...  ...  ...   ...\n",
              "57019           2      45589     64         1  ...      0.0  1.0  1.0   0.0\n",
              "57020           2      45589     64         1  ...      0.0  1.0  1.0   0.0\n",
              "57021           2      45589     64         1  ...      0.0  1.0  1.0   0.0\n",
              "57022           2      45589     64         1  ...      0.0  1.0  1.0   0.0\n",
              "57023           2      45589     64         1  ...      0.0  1.0  1.0   0.0\n",
              "\n",
              "[57024 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "YqedqOEpBZSB",
        "outputId": "9f2cf288-6c5a-4077-c07a-4bf9c3acb33c"
      },
      "source": [
        "NlpGrid['quality'] = model.predict(NlpGrid)\r\n",
        "NlpGrid"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_dim</th>\n",
              "      <th>embedding</th>\n",
              "      <th>nodes</th>\n",
              "      <th>stacking</th>\n",
              "      <th>dropout</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>epochs</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>n_features</th>\n",
              "      <th>dominant_class</th>\n",
              "      <th>relu</th>\n",
              "      <th>tanh</th>\n",
              "      <th>L1</th>\n",
              "      <th>L2</th>\n",
              "      <th>None</th>\n",
              "      <th>adam</th>\n",
              "      <th>rmsprop</th>\n",
              "      <th>sgd</th>\n",
              "      <th>GRU</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.345499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.345499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.265061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>10</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.265061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.265061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57019</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>45</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.205499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57020</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.205499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57021</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>50</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.205499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57022</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.205499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57023</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>55</td>\n",
              "      <td>64</td>\n",
              "      <td>360</td>\n",
              "      <td>17</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.205499</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>57024 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       output_dim  embedding  nodes  stacking  ...  sgd  GRU  LSTM   quality\n",
              "0               2      45589     32         0  ...  0.0  0.0   1.0  1.345499\n",
              "1               2      45589     32         0  ...  0.0  0.0   1.0  1.345499\n",
              "2               2      45589     32         0  ...  0.0  0.0   1.0  1.265061\n",
              "3               2      45589     32         0  ...  0.0  0.0   1.0  1.265061\n",
              "4               2      45589     32         0  ...  0.0  0.0   1.0  1.265061\n",
              "...           ...        ...    ...       ...  ...  ...  ...   ...       ...\n",
              "57019           2      45589     64         1  ...  1.0  1.0   0.0  1.205499\n",
              "57020           2      45589     64         1  ...  1.0  1.0   0.0  1.205499\n",
              "57021           2      45589     64         1  ...  1.0  1.0   0.0  1.205499\n",
              "57022           2      45589     64         1  ...  1.0  1.0   0.0  1.205499\n",
              "57023           2      45589     64         1  ...  1.0  1.0   0.0  1.205499\n",
              "\n",
              "[57024 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPwUy0qiGFVp"
      },
      "source": [
        "res = dict(NlpGrid.loc[NlpGrid['quality'] == max(NlpGrid['quality'])].iloc[0])\r\n",
        "\r\n",
        "if res['relu'] == 1:\r\n",
        "  activation = 'relu'\r\n",
        "if res['tanh'] == 1:\r\n",
        "  activation = 'tanh'\r\n",
        "if res['L1'] == 1:\r\n",
        "  regularizer = 'L1' \r\n",
        "if res['L2'] == 1:\r\n",
        "  regularizer = 'L2' \r\n",
        "if res['None'] == 1:\r\n",
        "  regularizer = 'None' \r\n",
        "if res['adam'] == 1:\r\n",
        "  optimizer = 'adam'\r\n",
        "if res['rmsprop'] == 1:\r\n",
        "  optimizer = 'rmsprop'\r\n",
        "if res['sgd'] == 1:\r\n",
        "  optimizer = 'sgd'\r\n",
        "if res['GRU'] == 1:\r\n",
        "  method = 'GRU'\r\n",
        "if res['LSTM'] == 1:\r\n",
        "  method = 'LSTM' "
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIBSFmEtQPQ_",
        "outputId": "9d1623f7-327c-4524-ba63-b7f1fe4a8a8c"
      },
      "source": [
        "def RNN(output_dim, embedding, nodes, activation, regularizer, stacking, dropout, optimizer, method, bidirectional, metrics='accuracy'):\r\n",
        "  if output_dim == 1:\r\n",
        "    loss = 'mse'\r\n",
        "    oa = 'linear'\r\n",
        "  if output_dim == 2:\r\n",
        "    loss = 'binary_crossentropy'\r\n",
        "    oa = 'sigmoid'\r\n",
        "  if output_dim >= 3:\r\n",
        "    loss = 'categorical_crossentropy'\r\n",
        "    oa = 'softmax'\r\n",
        "  model = models.Sequential()\r\n",
        "  model.add(layers.Embedding(embedding, nodes))\r\n",
        "  if method == 'LSTM':\r\n",
        "    if bidirectional == True:\r\n",
        "      if stacking == True:\r\n",
        "        model.add(layers.Bidirectional(layers.LSTM(nodes, activation=activation, return_sequences=True)))\r\n",
        "        model.add(layers.Bidirectional(layers.LSTM(nodes, activation=activation)))\r\n",
        "      else:\r\n",
        "        model.add(layers.Bidirectional(layers.LSTM(nodes, activation=activation)))\r\n",
        "    else:\r\n",
        "      if stacking == True:\r\n",
        "        model.add(layers.LSTM(nodes, activation=activation, return_sequences=True))\r\n",
        "        model.add(layers.LSTM(nodes, activation=activation))\r\n",
        "      else:\r\n",
        "        model.add(layers.LSTM(nodes, activation=activation))\r\n",
        "  if method == 'GRU':\r\n",
        "    if bidirectional == True:\r\n",
        "      if stacking == True:\r\n",
        "        model.add(layers.Bidirectional(layers.GRU(nodes, activation=activation, return_sequences=True)))\r\n",
        "        model.add(layers.Bidirectional(layers.GRU(nodes, activation=activation)))\r\n",
        "      else:\r\n",
        "        model.add(layers.Bidirectional(layers.GRU(nodes, activation=activation)))\r\n",
        "    else:\r\n",
        "      if stacking == True:\r\n",
        "        model.add(layers.GRU(nodes, activation=activation, return_sequences=True))\r\n",
        "        model.add(layers.GRU(nodes, activation=activation))\r\n",
        "      else:\r\n",
        "        model.add(layers.GRU(nodes, activation=activation))\r\n",
        "  if dropout == True:\r\n",
        "    model.add(layers.Dropout(0.5))\r\n",
        "  model.add(layers.Dense(nodes/2, activation=activation))\r\n",
        "  model.add(layers.Dense(nodes/4, activation=activation))\r\n",
        "  model.add(layers.Dense(output_dim, activation=oa))\r\n",
        "  model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\r\n",
        "  return model\r\n",
        "\r\n",
        "RNN(int(res['output_dim']), int(res['embedding']), int(res['nodes']), activation, regularizer, bool(res['stacking']), bool(res['dropout']), optimizer, method, bool(res['bidirectional'])).summary()"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_79\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_75 (Embedding)     (None, None, 32)          1458848   \n",
            "_________________________________________________________________\n",
            "bidirectional_101 (Bidirecti (None, None, 64)          16640     \n",
            "_________________________________________________________________\n",
            "bidirectional_102 (Bidirecti (None, 64)                24832     \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 16)                1040      \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 8)                 136       \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 2)                 18        \n",
            "=================================================================\n",
            "Total params: 1,501,514\n",
            "Trainable params: 1,501,514\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}