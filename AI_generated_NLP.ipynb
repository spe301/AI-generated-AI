{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AI_generated_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1dWgxcQQ2BpqXYrFd5bNE_uW684UVyVyn",
      "authorship_tag": "ABX9TyN1oquKtlu8pjH1QmB2ZWEq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spe301/AI-generated-AI/blob/main/AI_generated_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkPOituvoRXT",
        "outputId": "415e107d-171f-4df1-83a5-b44044cd1479"
      },
      "source": [
        "!pip install Potosnail==0.2.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Potosnail==0.2.1 in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: regex==2020.10.15 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (2020.10.15)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (3.3.4)\n",
            "Requirement already satisfied: urllib3==1.25.11 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.25.11)\n",
            "Requirement already satisfied: scikit-learn==0.22.2.post1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.22.2.post1)\n",
            "Requirement already satisfied: xgboost<=1.3.1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.90)\n",
            "Requirement already satisfied: statsmodels>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.12.2)\n",
            "Requirement already satisfied: pandas>=1.1.3 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.1.5)\n",
            "Requirement already satisfied: beautifulsoup4==4.9.3 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (4.9.3)\n",
            "Requirement already satisfied: tensorflow>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (2.4.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (0.11.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (1.19.5)\n",
            "Requirement already satisfied: lxml==4.6.1 in /usr/local/lib/python3.7/dist-packages (from Potosnail==0.2.1) (4.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (2.8.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (7.0.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.3.2->Potosnail==0.2.1) (0.10.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1->Potosnail==0.2.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2.post1->Potosnail==0.2.1) (1.0.1)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.12.0->Potosnail==0.2.1) (0.5.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.3->Potosnail==0.2.1) (2018.9)\n",
            "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /usr/local/lib/python3.7/dist-packages (from beautifulsoup4==4.9.3->Potosnail==0.2.1) (2.2)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.7.4.3)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.12)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.3.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.2.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.36.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.3.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.12.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.4.1)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.10.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.32.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (0.10.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (2.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.4.0->Potosnail==0.2.1) (1.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.27.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.3.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (1.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.0.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (4.2.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow>=2.4.0->Potosnail==0.2.1) (3.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apVtNy1KLP64"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "from potosnail import *\r\n",
        "\r\n",
        "ml = MachineLearning()\r\n",
        "dl = DeepLearning()\r\n",
        "dh = DataHelper()\r\n",
        "ev = Evaluater()\r\n",
        "al = Algorithms()\r\n",
        "wr = Wrappers()\r\n",
        "st = Stats()\r\n",
        "\r\n",
        "def SuperBear(df):\r\n",
        "  dh = DataHelper()\r\n",
        "  '''gets the nlp results dataset ready for modeling'''\r\n",
        "  df['regularizer'] = df['regularizer'].fillna('None')\r\n",
        "  df['stacking'] = df['stacking'].astype(int)\r\n",
        "  df['dropout'] = df['dropout'].astype(int)\r\n",
        "  df['bidirectional'] = df['bidirectional'].astype(int)\r\n",
        "  act = dh.OHE(df['activation'])\r\n",
        "  reg = dh.OHE(df['regularizer'])\r\n",
        "  opt = dh.OHE(df['optimizer'])\r\n",
        "  method = dh.OHE(df['method'])\r\n",
        "  df = df.drop(['activation', 'regularizer', 'optimizer', 'method'], axis='columns')\r\n",
        "  df = pd.concat([df, act, reg, opt, method], axis='columns')\r\n",
        "  df['val_loss'] = df['val_loss'].fillna(max(df['val_loss']))\r\n",
        "  df['loss'] = df['loss'].fillna(max(df['loss']))\r\n",
        "  return df"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "298ew3nYB7sw"
      },
      "source": [
        "df1 = pd.read_csv('https://raw.githubusercontent.com/spe301/AI-generated-AI/main/Data/NLP8.csv').drop(['Unnamed: 0'], axis='columns')\r\n",
        "nlp = SuperBear(df1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZDLpl8gOgPw"
      },
      "source": [
        "# EDA Questions\r\n",
        "\r\n",
        "1. is the difference in validation loss between 9,000 point datasets and 360 point datasets statistically signifigant?\r\n",
        "2. is the difference in 'difference of validation loss and loss' between dropout and no dropout statistically signifigant?\r\n",
        "3. Which model had the highest epochs and how many standard deviations away were it's accuracies from average (z score)\r\n",
        "4. How is n_nodes correlated for smaller datasets compared to larger datasets?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUHZv0CGOckf",
        "outputId": "6716e201-24bc-467f-d4f3-49036361313c"
      },
      "source": [
        "from scipy.stats import ttest_ind\r\n",
        "\r\n",
        "s1 = nlp.loc[nlp['len_dataset'] == 360]['val_loss'].sample(100)\r\n",
        "s2 = nlp.loc[nlp['len_dataset'] != 360]['val_loss'].sample(100)\r\n",
        "ttest_ind(s1, s2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_indResult(statistic=8.7231388195817, pvalue=1.106088544388026e-15)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_iO6nEuXK1G"
      },
      "source": [
        "validation loss is signifigantly lower with larger dataset\r\n",
        "\r\n",
        "conclusion: Deep NLP works better for larger datasets with thousands of datapoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlQcrC7vTvSG",
        "outputId": "5b8a45e0-263f-4696-a256-72e6406c7ea6"
      },
      "source": [
        "ldd = nlp.loc[nlp['dropout'] == 1]['val_loss'] - nlp.loc[nlp['dropout'] == 1]['loss']\r\n",
        "s1 = ldd.sample(100)\r\n",
        "ldno = nlp.loc[nlp['dropout'] == 0]['val_loss'] - nlp.loc[nlp['dropout'] == 0]['loss']\r\n",
        "s2 = ldno.sample(100)\r\n",
        "ttest_ind(s1, s2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ttest_indResult(statistic=-0.09511000937926599, pvalue=0.9243236333863147)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZRbMuVhbER2"
      },
      "source": [
        "We can accept the null hypothesis that dropout does not cause a signifigant change in loss\r\n",
        "\r\n",
        "conclusion: adding a dropout layer is unlikley to decrease overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6GXfgrOdu8o",
        "outputId": "3d360010-fdd3-40ef-c58a-9017371c62f9"
      },
      "source": [
        "acc50 = np.mean(list(nlp.loc[nlp['epochs'] == max(nlp['epochs'])]['accuracy']))\r\n",
        "val50 = np.mean(list(nlp.loc[nlp['epochs'] == max(nlp['epochs'])]['val_accuracy']))\r\n",
        "\r\n",
        "st.Z(nlp['accuracy'], acc50), st.Z(nlp['val_accuracy'], val50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-1.8442059083699316, -0.43293229648074893)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hywG2OgCeDLH",
        "outputId": "1c3fced9-3698-4867-a308-17b4b3bcd5a3"
      },
      "source": [
        "val50"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4987113395917047"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fHjK9_Md9ZZ"
      },
      "source": [
        "Models that were trained for 50 epochs had lower accuracies than average but were also less overfit.\r\n",
        "\r\n",
        "Conclusion: overfitting seems to be a major problem, setting epochs higher while figuring out how to increase accuracy may be well worth it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0b51TUxe_Ai",
        "outputId": "2539c24a-a6c2-4741-a6c4-baccaca08952"
      },
      "source": [
        "np.mean(nlp['epochs'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12.6425"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "QxqJbdJTfF4G",
        "outputId": "a895eff3-d512-4f94-8c21-622755936545"
      },
      "source": [
        "kpi_list = ['accuracy', 'loss', 'val_accuracy', 'val_loss']\r\n",
        "kpi = nlp[kpi_list]\r\n",
        "\r\n",
        "scores = []\r\n",
        "for i in range(len(nlp)):\r\n",
        "  ts = (1 - (kpi['loss'][i] / max(kpi['loss'])) + kpi['accuracy'][i])/2\r\n",
        "  vs = (1 - (kpi['val_loss'][i] / max(kpi['val_loss'])) + kpi['val_accuracy'][i])/2\r\n",
        "  score = (ts+vs) - abs(ts-vs)\r\n",
        "  scores.append(score)\r\n",
        "\r\n",
        "nlp2 = nlp.drop(kpi_list, axis='columns')\r\n",
        "nlp2['quality'] = scores\r\n",
        "nlp2"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_dim</th>\n",
              "      <th>embedding</th>\n",
              "      <th>nodes</th>\n",
              "      <th>stacking</th>\n",
              "      <th>dropout</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>epochs</th>\n",
              "      <th>batch_size</th>\n",
              "      <th>len_dataset</th>\n",
              "      <th>n_features</th>\n",
              "      <th>dominant_class</th>\n",
              "      <th>relu</th>\n",
              "      <th>tanh</th>\n",
              "      <th>L1</th>\n",
              "      <th>L2</th>\n",
              "      <th>None</th>\n",
              "      <th>adam</th>\n",
              "      <th>rmsprop</th>\n",
              "      <th>sgd</th>\n",
              "      <th>GRU</th>\n",
              "      <th>LSTM</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.338706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.361174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.361862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>64</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.387474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>360</td>\n",
              "      <td>1724</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.490305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.405428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.395079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.341054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.342763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>3</td>\n",
              "      <td>12300</td>\n",
              "      <td>256</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>32</td>\n",
              "      <td>8936</td>\n",
              "      <td>17</td>\n",
              "      <td>0.602954</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.402762</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 22 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      output_dim  embedding  nodes  stacking  ...  sgd  GRU  LSTM   quality\n",
              "0              3      12300     64         0  ...  0.0  1.0   0.0  1.338706\n",
              "1              3      12300     64         1  ...  0.0  1.0   0.0  1.361174\n",
              "2              3      12300     64         0  ...  0.0  1.0   0.0  1.361862\n",
              "3              3      12300     64         1  ...  0.0  1.0   0.0  1.387474\n",
              "4              2      45589     64         0  ...  0.0  1.0   0.0  0.490305\n",
              "...          ...        ...    ...       ...  ...  ...  ...   ...       ...\n",
              "1995           3      12300    256         0  ...  0.0  0.0   1.0  1.405428\n",
              "1996           3      12300    256         1  ...  0.0  0.0   1.0  1.395079\n",
              "1997           3      12300    256         1  ...  0.0  0.0   1.0  1.341054\n",
              "1998           3      12300    256         1  ...  0.0  0.0   1.0  1.342763\n",
              "1999           3      12300    256         1  ...  0.0  0.0   1.0  1.402762\n",
              "\n",
              "[2000 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBtcZVDg-DcZ"
      },
      "source": [
        "counts = list(np.unique(nlp2['nodes']))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "wS68-1RV9vlJ",
        "outputId": "2d3ca7f7-1694-4429-97b5-f9b33b75a21b"
      },
      "source": [
        "import seaborn as sns\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "smaller = nlp2.loc[nlp2['len_dataset']==360]\r\n",
        "bigger = nlp2.loc[nlp2['len_dataset']!=360]\r\n",
        "\r\n",
        "quals1 = []\r\n",
        "quals2 = []\r\n",
        "for count in counts:\r\n",
        "  avg_score1 = np.mean(smaller.loc[smaller['nodes'] == count]['quality'])\r\n",
        "  avg_score2 = np.mean(bigger.loc[bigger['nodes'] == count]['quality'])\r\n",
        "  quals1.append(avg_score1)\r\n",
        "  quals2.append(avg_score2)\r\n",
        "\r\n",
        "sns.barplot(counts, quals1)\r\n",
        "plt.title('small datasets')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'small datasets')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "_w4ErkX_BVgk",
        "outputId": "f2d36042-ecf7-4232-f048-dfcedc59922a"
      },
      "source": [
        "sns.barplot(counts, quals2)\r\n",
        "plt.title('large datasets')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'large datasets')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9wO1pmspmrW"
      },
      "source": [
        "train, val = dh.HoldOut(nlp2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nussxRqSpy8U",
        "outputId": "6e994bb9-1689-44c6-9c92-0db8ace35b12"
      },
      "source": [
        "kit = wr.WrapML(train, 'quality', 'regression', quiet=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75.49% accuracy, untuned model, raw data\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    2.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "86.37% accuracy, tuned model, raw data\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    2.0s finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "69.95% accuracy, tuned model, data is scaled with standard scaler\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
            "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-8.45% accuracy, tuned model, features have been reduced to 5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giZ2Wocj1LLs"
      },
      "source": [
        "model = list(kit.values())[0][0]\r\n",
        "X = list(kit.values())[0][1]\r\n",
        "y = list(kit.values())[0][2]\r\n",
        "Xval = list(kit.values())[0][3]\r\n",
        "yval = list(kit.values())[0][4]\r\n",
        "list(kit.values())[0][5]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjlE5wXx1M9P",
        "outputId": "b69665d2-39f1-40b1-ea37-ec502c57a578"
      },
      "source": [
        "model"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
              "                      max_features=None, max_leaf_nodes=None,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=1, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9C3qQqk2CA9",
        "outputId": "db99897a-5392-4ec2-ddac-af7e8a0b1f67"
      },
      "source": [
        "grid = {'max_depth': [11, 12, 13], 'max_leaf_nodes': [9, 12, 15], 'min_samples_leaf': [3, 4]}\r\n",
        "\r\n",
        "clf = ml.Optimize(model, grid, X, y)\r\n",
        "clf"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
            "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=12,\n",
              "                      max_features=None, max_leaf_nodes=15,\n",
              "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                      min_samples_leaf=3, min_samples_split=2,\n",
              "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                      random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itXCCOyP2RIl",
        "outputId": "7520056b-afcc-4150-f7e1-642adc79b890"
      },
      "source": [
        "ev.EvaluateRegressor(clf, X, Xval, y, yval)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     predicted    actual     error    %error\n",
              " 0     1.184367  1.138172  0.046195  0.960996\n",
              " 1     1.396522  1.411170  0.014648  1.037996\n",
              " 2     1.135288  1.135954  0.000667  0.058691\n",
              " 3     1.396522  1.368966  0.027556  0.980268\n",
              " 4     1.135288  1.160250  0.024963  2.151501\n",
              " ..         ...       ...       ...       ...\n",
              " 175   1.396522  1.357586  0.038936  0.972119\n",
              " 176   1.396522  1.326643  0.069879  0.949962\n",
              " 177   1.135288  1.247804  0.112517  9.017183\n",
              " 178   1.396522  1.413435  0.016914  1.196636\n",
              " 179   1.396522  1.401696  0.005174  0.369141\n",
              " \n",
              " [180 rows x 4 columns], 0.006998524759018363, 85.18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7mbd3qa44gz"
      },
      "source": [
        "X = np.array(X)\r\n",
        "Xval = np.array(Xval)\r\n",
        "y = np.array(y)\r\n",
        "yval = np.array(yval)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzF--3Yx5Knd",
        "outputId": "cff28259-346e-4cc5-c1e8-85e1cd1767bd"
      },
      "source": [
        "dm = dl.FastNN('regression', 'mse', output_dim=1, nodes=32, regularizer='L2')\r\n",
        "history = dm.fit(X, y, epochs=150, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "41/41 [==============================] - 1s 6ms/step - loss: 103674.3009 - val_loss: 2238.6355\n",
            "Epoch 2/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 774.6535 - val_loss: 15.8291\n",
            "Epoch 3/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 10.7478 - val_loss: 3.7940\n",
            "Epoch 4/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 5.0770 - val_loss: 2.9615\n",
            "Epoch 5/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.2240 - val_loss: 3.8137\n",
            "Epoch 6/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 4.2380 - val_loss: 2.6623\n",
            "Epoch 7/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 3.3170 - val_loss: 1.9239\n",
            "Epoch 8/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.6901 - val_loss: 1.7310\n",
            "Epoch 9/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.9739 - val_loss: 1.3874\n",
            "Epoch 10/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.8335 - val_loss: 1.4177\n",
            "Epoch 11/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 1.4224 - val_loss: 1.1101\n",
            "Epoch 12/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.0018 - val_loss: 0.9176\n",
            "Epoch 13/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.2821 - val_loss: 0.9376\n",
            "Epoch 14/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8812 - val_loss: 0.8454\n",
            "Epoch 15/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.8658 - val_loss: 0.7959\n",
            "Epoch 16/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7232 - val_loss: 0.9109\n",
            "Epoch 17/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.8280 - val_loss: 0.7909\n",
            "Epoch 18/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.8068 - val_loss: 0.8347\n",
            "Epoch 19/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6518 - val_loss: 0.7707\n",
            "Epoch 20/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7540 - val_loss: 0.9511\n",
            "Epoch 21/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.7395 - val_loss: 0.9064\n",
            "Epoch 22/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7400 - val_loss: 0.8271\n",
            "Epoch 23/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6965 - val_loss: 0.7277\n",
            "Epoch 24/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6895 - val_loss: 0.6952\n",
            "Epoch 25/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7552 - val_loss: 0.7103\n",
            "Epoch 26/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 1.1088\n",
            "Epoch 27/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7726 - val_loss: 0.6710\n",
            "Epoch 28/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6383 - val_loss: 1.0009\n",
            "Epoch 29/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7789 - val_loss: 0.8452\n",
            "Epoch 30/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.7143 - val_loss: 0.6654\n",
            "Epoch 31/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6979 - val_loss: 0.6530\n",
            "Epoch 32/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6415 - val_loss: 0.7832\n",
            "Epoch 33/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6948 - val_loss: 0.6262\n",
            "Epoch 34/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.5993 - val_loss: 0.6252\n",
            "Epoch 35/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6211 - val_loss: 0.6096\n",
            "Epoch 36/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6673 - val_loss: 0.8115\n",
            "Epoch 37/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6759 - val_loss: 0.6802\n",
            "Epoch 38/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5343 - val_loss: 0.5757\n",
            "Epoch 39/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.5450\n",
            "Epoch 40/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5366 - val_loss: 0.5762\n",
            "Epoch 41/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5954 - val_loss: 0.7692\n",
            "Epoch 42/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6331 - val_loss: 0.5041\n",
            "Epoch 43/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5385 - val_loss: 0.5131\n",
            "Epoch 44/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5673 - val_loss: 0.6080\n",
            "Epoch 45/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5405 - val_loss: 0.5403\n",
            "Epoch 46/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5380 - val_loss: 0.6942\n",
            "Epoch 47/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5984 - val_loss: 0.6001\n",
            "Epoch 48/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4575 - val_loss: 0.4467\n",
            "Epoch 49/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4860 - val_loss: 0.5829\n",
            "Epoch 50/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4813 - val_loss: 0.7364\n",
            "Epoch 51/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5364 - val_loss: 0.4853\n",
            "Epoch 52/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4631 - val_loss: 0.7525\n",
            "Epoch 53/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6039 - val_loss: 0.5923\n",
            "Epoch 54/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.5265 - val_loss: 0.5604\n",
            "Epoch 55/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4766 - val_loss: 0.4436\n",
            "Epoch 56/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4236 - val_loss: 0.4198\n",
            "Epoch 57/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3619 - val_loss: 0.3794\n",
            "Epoch 58/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3328 - val_loss: 0.3646\n",
            "Epoch 59/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3542 - val_loss: 0.3128\n",
            "Epoch 60/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3564 - val_loss: 0.3048\n",
            "Epoch 61/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3611 - val_loss: 0.3394\n",
            "Epoch 62/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3948 - val_loss: 0.2871\n",
            "Epoch 63/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3350 - val_loss: 0.2811\n",
            "Epoch 64/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2768 - val_loss: 0.3187\n",
            "Epoch 65/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.8120 - val_loss: 1.3476\n",
            "Epoch 66/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 4.6306 - val_loss: 0.3333\n",
            "Epoch 67/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3383 - val_loss: 0.2927\n",
            "Epoch 68/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.3032 - val_loss: 0.2429\n",
            "Epoch 69/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2547 - val_loss: 0.2248\n",
            "Epoch 70/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2422 - val_loss: 0.2173\n",
            "Epoch 71/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2663 - val_loss: 0.2786\n",
            "Epoch 72/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2155 - val_loss: 0.2008\n",
            "Epoch 73/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2161 - val_loss: 0.4133\n",
            "Epoch 74/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2781 - val_loss: 0.2613\n",
            "Epoch 75/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2867 - val_loss: 0.2213\n",
            "Epoch 76/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2204 - val_loss: 0.2151\n",
            "Epoch 77/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2032 - val_loss: 0.1988\n",
            "Epoch 78/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1668 - val_loss: 0.2313\n",
            "Epoch 79/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1837 - val_loss: 0.2140\n",
            "Epoch 80/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2355 - val_loss: 0.2021\n",
            "Epoch 81/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2228 - val_loss: 0.2949\n",
            "Epoch 82/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2422 - val_loss: 0.2775\n",
            "Epoch 83/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2178 - val_loss: 0.1390\n",
            "Epoch 84/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1584 - val_loss: 0.1373\n",
            "Epoch 85/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1622 - val_loss: 0.1523\n",
            "Epoch 86/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1442 - val_loss: 0.1517\n",
            "Epoch 87/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2225 - val_loss: 0.1242\n",
            "Epoch 88/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1754 - val_loss: 0.1565\n",
            "Epoch 89/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1437 - val_loss: 0.1405\n",
            "Epoch 90/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1340 - val_loss: 0.1282\n",
            "Epoch 91/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1587 - val_loss: 0.1481\n",
            "Epoch 92/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1726 - val_loss: 0.2237\n",
            "Epoch 93/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1845 - val_loss: 0.1172\n",
            "Epoch 94/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1335 - val_loss: 0.1664\n",
            "Epoch 95/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1393 - val_loss: 0.1177\n",
            "Epoch 96/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1497 - val_loss: 0.3951\n",
            "Epoch 97/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2112 - val_loss: 0.2698\n",
            "Epoch 98/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1545 - val_loss: 0.1007\n",
            "Epoch 99/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1263 - val_loss: 0.4097\n",
            "Epoch 100/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2316 - val_loss: 0.3642\n",
            "Epoch 101/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1701 - val_loss: 0.1109\n",
            "Epoch 102/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1646 - val_loss: 0.1153\n",
            "Epoch 103/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1016 - val_loss: 0.1309\n",
            "Epoch 104/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1578 - val_loss: 0.0918\n",
            "Epoch 105/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1700 - val_loss: 0.3684\n",
            "Epoch 106/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2558 - val_loss: 1.0358\n",
            "Epoch 107/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2828 - val_loss: 0.0898\n",
            "Epoch 108/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1287 - val_loss: 0.0889\n",
            "Epoch 109/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1372 - val_loss: 0.1464\n",
            "Epoch 110/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.6560 - val_loss: 0.5925\n",
            "Epoch 111/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3193 - val_loss: 1.2826\n",
            "Epoch 112/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.8502 - val_loss: 1.6395\n",
            "Epoch 113/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.9187 - val_loss: 0.3611\n",
            "Epoch 114/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6357 - val_loss: 0.7659\n",
            "Epoch 115/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3538 - val_loss: 0.2283\n",
            "Epoch 116/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1634 - val_loss: 0.0844\n",
            "Epoch 117/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1463 - val_loss: 0.0997\n",
            "Epoch 118/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4010 - val_loss: 0.1347\n",
            "Epoch 119/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2323 - val_loss: 1.1943\n",
            "Epoch 120/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.8026 - val_loss: 0.6019\n",
            "Epoch 121/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.9109 - val_loss: 0.8113\n",
            "Epoch 122/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 9.0308 - val_loss: 233.4889\n",
            "Epoch 123/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 149.6869 - val_loss: 16.3759\n",
            "Epoch 124/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 8.3054 - val_loss: 9.4668\n",
            "Epoch 125/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 8.3740 - val_loss: 4.8203\n",
            "Epoch 126/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 2.2872 - val_loss: 0.1217\n",
            "Epoch 127/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.3440 - val_loss: 0.1596\n",
            "Epoch 128/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.2421 - val_loss: 0.3790\n",
            "Epoch 129/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.5330 - val_loss: 0.0798\n",
            "Epoch 130/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1100 - val_loss: 0.3404\n",
            "Epoch 131/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.4135 - val_loss: 0.0826\n",
            "Epoch 132/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.2480 - val_loss: 0.1227\n",
            "Epoch 133/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1264 - val_loss: 0.1436\n",
            "Epoch 134/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1616 - val_loss: 0.3013\n",
            "Epoch 135/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1479 - val_loss: 0.0777\n",
            "Epoch 136/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1392 - val_loss: 3.1520\n",
            "Epoch 137/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.6543 - val_loss: 0.1019\n",
            "Epoch 138/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.0396 - val_loss: 5.4859\n",
            "Epoch 139/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 1.3132 - val_loss: 4.9596\n",
            "Epoch 140/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 13.6044 - val_loss: 638.4244\n",
            "Epoch 141/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 426.5755 - val_loss: 5.5850\n",
            "Epoch 142/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 23.1368 - val_loss: 0.6054\n",
            "Epoch 143/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.6587 - val_loss: 0.1262\n",
            "Epoch 144/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1162 - val_loss: 0.0872\n",
            "Epoch 145/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.0903 - val_loss: 0.0955\n",
            "Epoch 146/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.0911 - val_loss: 0.0742\n",
            "Epoch 147/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.0830 - val_loss: 0.1444\n",
            "Epoch 148/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.1003 - val_loss: 0.0961\n",
            "Epoch 149/150\n",
            "41/41 [==============================] - 0s 3ms/step - loss: 0.0938 - val_loss: 0.1363\n",
            "Epoch 150/150\n",
            "41/41 [==============================] - 0s 2ms/step - loss: 0.1129 - val_loss: 0.0793\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pkE1KbW55Nj",
        "outputId": "97a6604e-7053-4aa2-c18a-17f1a8dbfc31"
      },
      "source": [
        "dm.evaluate(Xval, yval)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 2ms/step - loss: 0.0813\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08125180751085281"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "hmZf6qwR6Fsa",
        "outputId": "d33fdbc3-cab1-4bbf-9404-a71f7a1f47a0"
      },
      "source": [
        "ev.ViewLoss(history)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAEGCAYAAACkQqisAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAc8ElEQVR4nO3df5BV5Z3n8fenLy1IREEgynSTgYzUGDQRI1GyZqYs3TFgVJzxB7omcTPu8EdMRbNJZnBMrUnGVExldpyxVjNjIglmjcTBGNkshqiBWNn4C4w/wF+gYmjCjxYBYQSk4bt/nKe77+3bTZ976dv3kv68qrr6nuecc++3j3Z/OM85z3MUEZiZmVWjqd4FmJnZ4cshYmZmVXOImJlZ1RwiZmZWNYeImZlVbVi9Cxhs48aNi0mTJtW7DDOzw8bKlSvfjIjxva0bciEyadIkVqxYUe8yzMwOG5Le6Gudu7PMzKxqDhEzM6uaQ8TMzKo25K6JmJlVat++fbS1tbFnz556l1JTI0aMoLW1lebm5tz7OETMzPrR1tbGqFGjmDRpEpLqXU5NRARbt26lra2NyZMn597P3VlmZv3Ys2cPY8eO/YMNEABJjB07tuKzLYeImVkOf8gB0qman9EhktOtj6zhV6+017sMM7OG4hDJ6TvLX+XXaxwiZjb4tm/fzu23317xfueddx7bt28f+IKKOERyKjSJA35+l5nVQV8h0tHRcdD9lixZwujRo2tUVcZ3Z+UkwX6niJnVwbx583j11VeZNm0azc3NjBgxgjFjxvDSSy/xyiuvcNFFF7F+/Xr27NnDtddey9y5c4HuaZ527drFrFmz+NjHPsZvfvMbWlpaeOCBBzjyyCMPuTaHSE7ZmYhDxGyo+9r/Wc0Lv397QN9z6h8dzY0XnNTn+ptvvplVq1bxzDPPsHz5cj7xiU+watWqrltx58+fz7HHHsvu3bv5yEc+wsUXX8zYsWNL3mPNmjXcc889fPe73+Wyyy7jvvvu45Of/OQh1+4Qyakgh4iZNYbTTz+9ZCzHrbfeyv333w/A+vXrWbNmTVmITJ48mWnTpgFw2mmnsW7dugGppeYhIqkArAA2RMT5kiYDC4GxwErgUxHxrqThwF3AacBWYE5ErEvvcT1wNbAf+HxELE3tM4F/AQrA9yLi5hr+HOw/UKt3N7PDxcHOGAbLe97znq7Xy5cv5+GHH+axxx5j5MiRnHXWWb2O9Rg+fHjX60KhwO7duweklsG4sH4t8GLR8reAWyLiBGAbWTiQvm9L7bek7ZA0FbgcOAmYCdwuqZDC6TZgFjAVuCJtWxOFJjjgayJmVgejRo1i586dva7bsWMHY8aMYeTIkbz00ks8/vjjg1pbTUNEUivwCeB7aVnA2cCitMkC4KL0enZaJq0/J20/G1gYEXsj4nVgLXB6+lobEa9FxLtkZzeza/WzuDvLzOpl7NixnHnmmZx88sl8+ctfLlk3c+ZMOjo6+MAHPsC8efOYMWPGoNZW6+6sfwb+FhiVlscC2yOi8760NqAlvW4B1gNERIekHWn7FqA4Wov3Wd+j/YwBrr+LJPY7RMysTn70ox/12j58+HAefPDBXtd1XvcYN24cq1at6mr/0pe+NGB11exMRNL5wJaIWFmrz6iglrmSVkha0d5e3YDBQpPcnWVm1kMtu7POBC6UtI6sq+lssovgoyV1ngG1AhvS6w3ARIC0/hiyC+xd7T326au9TETcERHTI2L6+PG9Pia4Xx5saGZWrmYhEhHXR0RrREwiuzD+y4i4ElgGXJI2uwp4IL1enJZJ638ZEZHaL5c0PN3ZNQV4EngKmCJpsqQj0mcsrtXPI+HuLDOzHuoxTuTvgIWSbgJ+C9yZ2u8EfihpLfAWWSgQEasl3Qu8AHQA10TEfgBJnwOWkt3iOz8iVteq6ILcnWVm1tOghEhELAeWp9evkd1Z1XObPcClfez/DeAbvbQvAZYMYKl98oh1M7NynoAxJw82NDMr5xDJqdCEz0TM7LBw1FFHDdpnOURy8mBDM7NynoAxp6w7yyFiZoNv3rx5TJw4kWuuuQaAr371qwwbNoxly5axbds29u3bx0033cTs2TWbtKNPDpGcfGHdzAB4cB5sen5g3/P4D8KsvuePnTNnDtddd11XiNx7770sXbqUz3/+8xx99NG8+eabzJgxgwsvvHDQnwXvEMkpu8W33lWY2VB06qmnsmXLFn7/+9/T3t7OmDFjOP744/nCF77Ao48+SlNTExs2bGDz5s0cf/zxg1qbQyQnDzY0M+CgZwy1dOmll7Jo0SI2bdrEnDlzuPvuu2lvb2flypU0NzczadKkXqeArzVfWM/Jc2eZWT3NmTOHhQsXsmjRIi699FJ27NjBe9/7Xpqbm1m2bBlvvPFGXerymUhOviZiZvV00kknsXPnTlpaWpgwYQJXXnklF1xwAR/84AeZPn06J554Yl3qcojklE0FX+8qzGwoe/757gv648aN47HHHut1u127dg1WSe7OyqsgP9nQzKwnh0hO7s4yMyvnEMnJgw3NhrYYAv+IrOZndIjk5GlPzIauESNGsHXr1j/oIIkItm7dyogRIyrazxfWc/KTDc2GrtbWVtra2qj28dqHixEjRtDa2lrRPg6RnOQL62ZDVnNzM5MnT653GQ3J3Vk5FZrkEetmZj04RHLyNREzs3IOkZzkCRjNzMo4RHIqNOFbfM3MenCI5OTBhmZm5RwiOcnXRMzMyjhEcip4xLqZWRmHSE4ebGhmVs4hkpMHG5qZlXOI5FSQBxuamfXkEMnJd2eZmZVziOTkwYZmZuUcIjkVmnB3lplZDw6RnDx3lplZOYdITpKIGBpPNzMzy8shklOhSYDnzzIzK+YQyakzRJwhZmbdHCI5KcsQXxcxMyviEMmpIHdnmZn15BDJqbs7yyFiZtbJIZKT0pmIBxyamXVziORUSNdEPODQzKybQyQnd2eZmZWrWYhIGiHpSUnPSlot6WupfbKkJyStlfRjSUek9uFpeW1aP6nova5P7S9L+nhR+8zUtlbSvFr9LOmzAE8Hb2ZWrJZnInuBsyPiFGAaMFPSDOBbwC0RcQKwDbg6bX81sC2135K2Q9JU4HLgJGAmcLukgqQCcBswC5gKXJG2rYmuwYY+EzEz61KzEInMrrTYnL4COBtYlNoXABel17PTMmn9Ocr++T8bWBgReyPidWAtcHr6WhsRr0XEu8DCtG1NdN7i6xMRM7NuNb0mks4YngG2AA8BrwLbI6IjbdIGtKTXLcB6gLR+BzC2uL3HPn2191bHXEkrJK1ob2+v8mfJvrs7y8ysW01DJCL2R8Q0oJXszOHEWn7eQeq4IyKmR8T08ePHV/UenjvLzKzcoNydFRHbgWXAR4HRkoalVa3AhvR6AzARIK0/Btha3N5jn77aa8J3Z5mZlavl3VnjJY1Or48E/gJ4kSxMLkmbXQU8kF4vTsuk9b+MbN71xcDl6e6tycAU4EngKWBKutvrCLKL74tr+PMADhEzs2LD+t+kahOABekuqibg3oj4maQXgIWSbgJ+C9yZtr8T+KGktcBbZKFARKyWdC/wAtABXBMR+wEkfQ5YChSA+RGxulY/TPfcWbX6BDOzw0/NQiQingNO7aX9NbLrIz3b9wCX9vFe3wC+0Uv7EmDJIRebQyGds/lMxMysm0es5yTP4mtmVsYhklPB10TMzMo4RHLykw3NzMo5RHLqHGzo7iwzs24OkZw8TsTMrJxDJKeCZ/E1MyvjEMmp6+4sn4mYmXVxiOTU1Z3lwYZmZl0cIjl5sKGZWTmHSE7uzjIzK+cQyckX1s3MyjlEcvJgQzOzcg6RnDzY0MysnEMkJw82NDMr5xDJyRMwmpmVc4jk5KngzczKOURycneWmVk5h0hO3bf41rkQM7MG4hDJqevuLJ+JmJl1cYjk1D13lkPEzKyTQyQnDzY0MyvnEMnJ3VlmZuUcIjl57iwzs3IOkZx8i6+ZWTmHSE4ebGhmVs4hkpPPRMzMyjlEcuqeO6vOhZiZNRCHSE6eCt7MrJxDJCcPNjQzK+cQycndWWZm5RwiOXmwoZlZuVwhIulaSUcrc6ekpyWdW+viGokkmuTuLDOzYnnPRP46It4GzgXGAJ8Cbq5ZVQ2q0CTf4mtmViRviKTOHM4DfhgRq4vahgxJ7s4yMyuSN0RWSvoFWYgslTQKGHKPZypI7s4yMysyLOd2VwPTgNci4h1JxwKfqVlVDSrrzqp3FWZmjSPvmchHgZcjYrukTwJfAXbUrqzGJHmwoZlZsbwh8h3gHUmnAF8EXgXuqllVDcoX1s3MSuUNkY6ICGA28L8i4jZg1MF2kDRR0jJJL0haLena1H6spIckrUnfx6R2SbpV0lpJz0n6cNF7XZW2XyPpqqL20yQ9n/a5VZ1T7dZIQQ4RM7NieUNkp6TryW7t/b+SmoDmfvbpAL4YEVOBGcA1kqYC84BHImIK8EhaBpgFTElfc8nOfkjXX24EzgBOB27sDJ60zd8U7Tcz589TFUnsH3K3E5iZ9S1viMwB9pKNF9kEtALfPtgOEbExIp5Or3cCLwItZGczC9JmC4CL0uvZwF2ReRwYLWkC8HHgoYh4KyK2AQ8BM9O6oyPi8XSWdFfRe9VEocmDDc3MiuUKkRQcdwPHSDof2BMRua+JSJoEnAo8ARwXERvTqk3Acel1C7C+aLe21Haw9rZe2nv7/LmSVkha0d7enrfsMu7OMjMrlXfak8uAJ4FLgcuAJyRdknPfo4D7gOvSqPcu6Qyi5n+VI+KOiJgeEdPHjx9f9ft4sKGZWam840RuAD4SEVsAJI0HHgYWHWwnSc1kAXJ3RPwkNW+WNCEiNqYuqS2pfQMwsWj31tS2ATirR/vy1N7ay/Y1U2jyYEMzs2J5r4k0dQZIsrW/fdOdUncCL0bEPxWtWgx03mF1FfBAUfun011aM4AdqdtrKXCupDHpgvq5wNK07m1JM9JnfbrovWrCgw3NzErlPRP5uaSlwD1peQ6wpJ99ziS7m+t5Sc+ktr8nm7jxXklXA2+QdY+R3u88YC3wDmlEfES8JekfgKfSdl+PiLfS688CPwCOBB5MXzUjeSp4M7NiuUIkIr4s6WKyYAC4IyLu72efX9P3JI3n9LJ9ANf08V7zgfm9tK8ATj5YHQPJc2eZmZXKeyZCRNxHdn1jyPKIdTOzUgcNEUk76f3uKZGdPBxdk6oalAcbmpmVOmiIRMRBpzYZagpN+EzEzKyIn7FeAQ82NDMr5RCpQNad5RAxM+vkEKmAL6ybmZVyiFQgu8W33lWYmTUOh0gFPNjQzKyUQ6QCnjvLzKyUQ6QCviZiZlbKIVKBbCr4eldhZtY4HCIVKMhPNjQzK+YQqYC7s8zMSjlEKuDBhmZmpRwiFfC0J2ZmpRwiFfCTDc3MSjlEKiBfWDczK+EQqUChSR6xbmZWxCFSAV8TMTMr5RCpgDwBo5lZCYdIBQpN+BZfM7MiDpEKeLChmVkph0gF5GsiZmYlHCIVKHjEuplZCYdIBTzY0MyslEOkAh5saGZWyiFSgYI82NDMrJhDpAK+O8vMrJRDpAIebGhmVsohUoFCE+7OMjMr4hCpgOfOMjMr5RCpgCQiIBwkZmaAQ6QihSYBnj/LzKyTQ6QCnSHiDDEzyzhEKqAsQ3xdxMwscYhUoCB3Z5mZFXOIVKC7O8shYmYGDpGKKJ2JeMChmVmmZiEiab6kLZJWFbUdK+khSWvS9zGpXZJulbRW0nOSPly0z1Vp+zWSripqP03S82mfW9X5F76GCukTPODQzCxTyzORHwAze7TNAx6JiCnAI2kZYBYwJX3NBb4DWegANwJnAKcDN3YGT9rmb4r26/lZA87dWWZmpWoWIhHxKPBWj+bZwIL0egFwUVH7XZF5HBgtaQLwceChiHgrIrYBDwEz07qjI+LxyEb+3VX0XjXT3Z3lEDEzg8G/JnJcRGxMrzcBx6XXLcD6ou3aUtvB2tt6aa+prsGGPhMxMwPqeGE9nUEMyl9jSXMlrZC0or29ver36bzF1yciZmaZwQ6RzakrivR9S2rfAEws2q41tR2svbWX9l5FxB0RMT0ipo8fP77q4rsGGzpFzMyAwQ+RxUDnHVZXAQ8UtX863aU1A9iRur2WAudKGpMuqJ8LLE3r3pY0I92V9emi96oZz51lZlZqWK3eWNI9wFnAOEltZHdZ3QzcK+lq4A3gsrT5EuA8YC3wDvAZgIh4S9I/AE+l7b4eEZ0X6z9LdgfYkcCD6aumfHeWmVmpmoVIRFzRx6pzetk2gGv6eJ/5wPxe2lcAJx9KjZXqujvLIWJmBnjEekW6586qcyFmZg3CIVKBQjpaPhMxM8s4RCogz+JrZlbCIVKBgq+JmJmVcIhUwE82NDMr5RCpQOdgQ3dnmZllHCIV8DgRM7NSDpEKFDyLr5lZCYdIBbruzvKZiJkZ4BCpSFd3lgcbmpkBDpGKeLChmVkph0gF3J1lZlbKIVIBX1g3MyvlEKmABxuamZVyiFTAgw3NzEo5RCrgwYZmZqUcIhXwBIxmZqUcIhXwVPBmZqUcIhVwd5aZWSmHSAW6b/GtcyFmZg3CIVKBrruzfCZiZgY4RCrSPXeWQ8TMDBwiFfFgQzOzUg6RCrg7y8yslEOkAp47y8yslEOkAr7F18yslEOkAh5saGZWyiFSAZ+JmJmVcohUoHvurDoXYmbWIBwiFfBU8GZmpYbVu4DDwv598Ow9DDv2BMB3Z5mZdfKZSB5Nw2DpVyg8/++Au7PMzDo5RPKQYMKHYNOzgAcbmpl1cojkNeEUtHk1zdrv7iwzs8QhkteEU6BjD1OaNvoWXzOzxCGS14RTADi5aZ27s8zMEodIXmNPgOaRnKzX3Z1lZpY4RPJqKsBxJzNV63x3lplZ4hCpxIRTOFHrOLB/f70rMbM/VHt3wT1XwIs/q3cluRz2ISJppqSXJa2VNK+mHzbhFI5iN0fvaavpx5jZEPaLr8DLS+Cnn2Xdqy+za29HvSs6qMM6RCQVgNuAWcBU4ApJU2v2geni+hurfsN9Kx0kZoe91x+F75/Hvvnn89tf/ZTlL23mP+r5R3vtw7Dy++ybegl7973L735wNef+z+U8+kp7/Wrqh+IwvtNI0keBr0bEx9Py9QAR8c2+9pk+fXqsWLGiug/seJf4Zgu7Dwxj+4ER0DSM/RQIVFndvbSFKnuP3lRah9lQNiw6aIlNbGQciv0cr21sjVHsp4CamgjEAbLvnV/Fy10EovzvaNcWZX9jg+hcH51LmXFsZyNj+cv93+SCA8v5RvN8Nms8u/c30dQkkFB65+6/GZ31Hdw7hWOYesP/y318Sn4WaWVETO9t3eE+d1YLsL5ouQ04o+dGkuYCcwHe9773Vf9pw45A532bERueZsfG7ex6Zw+F6ICe//mi15d9bXKQrfLTAPxjoP//Dc3+sDw18mJ+e9xf0TJ6JH/x7kOM2Poim9/eze69+7piQ9H9nc62FAPF/wjv+qPe4/dfUnegdL0W3f9u7H79ZlMzvz7uSi4eOZkLPvRn8LtWxm1+kVe37OSdvR3ZTT1xgANZ+qQ68kQIdDQfXe1hOqjD/UzkEmBmRPy3tPwp4IyI+Fxf+xzSmYiZ2RB0sDORw/qaCLABmFi03JrazMxsEBzuIfIUMEXSZElHAJcDi+tck5nZkHFYXxOJiA5JnwOWAgVgfkSsrnNZZmZDxmEdIgARsQRYUu86zMyGosO9O8vMzOrIIWJmZlVziJiZWdUcImZmVrXDerBhNSS1A29Uufs44M0BLKcWXOOha/T6wDUOFNeYzx9HxPjeVgy5EDkUklb0NWqzUbjGQ9fo9YFrHCiu8dC5O8vMzKrmEDEzs6o5RCpzR70LyME1HrpGrw9c40BxjYfI10TMzKxqPhMxM7OqOUTMzKxqDpEcJM2U9LKktZLm1bseAEkTJS2T9IKk1ZKuTe3HSnpI0pr0fUwD1FqQ9FtJP0vLkyU9kY7nj9M0/vWsb7SkRZJekvSipI822nGU9IX033mVpHskjaj3cZQ0X9IWSauK2no9bsrcmmp9TtKH61jjt9N/6+ck3S9pdNG661ONL0v6eD3qK1r3RUkhaVxarssx7I9DpB+SCsBtwCxgKnCFpKn1rQqADuCLETEVmAFck+qaBzwSEVOAR9JyvV0LvFi0/C3glog4AdgGXF2Xqrr9C/DziDgROIWs1oY5jpJagM8D0yPiZLLHHlxO/Y/jD4CZPdr6Om6zgCnpay7wnTrW+BBwckR8CHgFuB4g/f5cDpyU9rk9/f4Pdn1ImgicC/yuqLlex/CgHCL9Ox1YGxGvRcS7wEJgdp1rIiI2RsTT6fVOsj98LWS1LUibLQAuqkuBiaRW4BPA99KygLOBRWmTutYo6Rjgz4E7ASLi3YjYToMdR7LHNhwpaRgwEthInY9jRDwKvNWjua/jNhu4KzKPA6MlTahHjRHxi4joSIuPkz0RtbPGhRGxNyJeB9aS/f4Pan3JLcDfUvLE9vocw/44RPrXAqwvWm5LbQ1D0iTgVOAJ4LiI2JhWbQKOq1ddyT+T/TIcSMtjge1Fv8T1Pp6TgXbg+6nL7XuS3kMDHceI2AD8I9m/SjcCO4CVNNZx7NTXcWvU36O/Bh5MrxuiRkmzgQ0R8WyPVQ1RX08OkcOcpKOA+4DrIuLt4nWR3b9dt3u4JZ0PbImIlfWqIYdhwIeB70TEqcB/0KPrqgGO4xiyf4VOBv4IeA+9dIE0mnoft/5IuoGsW/juetfSSdJI4O+B/1HvWvJyiPRvAzCxaLk1tdWdpGayALk7In6Smjd3nuKm71vqVR9wJnChpHVk3YBnk11/GJ26ZaD+x7MNaIuIJ9LyIrJQaaTj+J+B1yOiPSL2AT8hO7aNdBw79XXcGur3SNJ/Bc4HrozuwXKNUOOfkP1j4dn0e9MKPC3p+Aapr4xDpH9PAVPSnTBHkF14W1znmjqvLdwJvBgR/1S0ajFwVXp9FfDAYNfWKSKuj4jWiJhEdtx+GRFXAsuAS9Jm9a5xE7Be0p+mpnOAF2ig40jWjTVD0sj0372zxoY5jkX6Om6LgU+nO4xmADuKur0GlaSZZF2sF0bEO0WrFgOXSxouaTLZBewnB7O2iHg+It4bEZPS700b8OH0/2nDHMMSEeGvfr6A88ju4ngVuKHe9aSaPkbWVfAc8Ez6Oo/smsMjwBrgYeDYetea6j0L+Fl6/X6yX861wL8Dw+tc2zRgRTqWPwXGNNpxBL4GvASsAn4IDK/3cQTuIbtGs4/sj93VfR03QGR3Ob4KPE92p1m9alxLdm2h8/fmX4u2vyHV+DIwqx719Vi/DhhXz2PY35enPTEzs6q5O8vMzKrmEDEzs6o5RMzMrGoOETMzq5pDxMzMquYQMWtwks5SmgHZrNE4RMzMrGoOEbMBIumTkp6U9Iykf1P2HJVdkm5JzwJ5RNL4tO00SY8XPdOi87kbJ0h6WNKzkp6W9Cfp7Y9S9zNP7k4j15F0s7Jnyjwn6R/r9KPbEOYQMRsAkj4AzAHOjIhpwH7gSrLJEldExEnAr4Ab0y53AX8X2TMtni9qvxu4LSJOAf4T2WhmyGZpvo7smTbvB86UNBb4S+Ck9D431fJnNOuNQ8RsYJwDnAY8JemZtPx+sinwf5y2+d/Ax9IzTEZHxK9S+wLgzyWNAloi4n6AiNgT3XM7PRkRbRFxgGyqjklkU8LvAe6U9FdA8TxQZoPCIWI2MAQsiIhp6etPI+KrvWxX7TxDe4te7weGRfYskdPJZh4+H/h5le9tVjWHiNnAeAS4RNJ7oetZ439M9jvWOdPufwF+HRE7gG2S/iy1fwr4VWRPqGyTdFF6j+Hp+RK9Ss+SOSYilgBfIHu0r9mgGtb/JmbWn4h4QdJXgF9IaiKblfUasodcnZ7WbSG7bgLZNOn/mkLiNeAzqf1TwL9J+np6j0sP8rGjgAckjSA7E/rvA/xjmfXLs/ia1ZCkXRFxVL3rMKsVd2eZmVnVfCZiZmZV85mImZlVzSFiZmZVc4iYmVnVHCJmZlY1h4iZmVXt/wN79NYQoJbTrgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEQDL3giL_7Y",
        "outputId": "9a2e2b39-554e-43d8-903e-612182886cc9"
      },
      "source": [
        "[0.5] * 10"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "nx5SkL8PB-WR",
        "outputId": "9e4a9d43-3340-4fe3-ccc8-8d29201ab121"
      },
      "source": [
        "def BunnyPinkears(params, len_dataset, n_features, dominant_class):\r\n",
        "  '''puts all possible gridsearch combinations in a dataframe'''\r\n",
        "  n = list(params.keys())\r\n",
        "  lst1 = []\r\n",
        "  lst2 = []\r\n",
        "  lst3 = []\r\n",
        "  lst4 = []\r\n",
        "  lst5 = []\r\n",
        "  lst6 = []\r\n",
        "  lst7 = []\r\n",
        "  lst8 = []\r\n",
        "  lst9 = []\r\n",
        "  lst10 = []\r\n",
        "  for i in range(len(params[n[0]])):\r\n",
        "    var1 = params[n[0]][i]\r\n",
        "    for i in range(len(params[n[1]])):\r\n",
        "      var2 = params[n[1]][i]\r\n",
        "      for i in range(len(params[n[2]])):\r\n",
        "        var3 = params[n[2]][i]\r\n",
        "        for i in range(len(params[n[3]])):\r\n",
        "          var4 = params[n[3]][i]\r\n",
        "          for i in range(len(params[n[4]])):\r\n",
        "            var5 = params[n[4]][i]\r\n",
        "            for i in range(len(params[n[5]])):\r\n",
        "              var6 = params[n[5]][i]\r\n",
        "              for i in range(len(params[n[6]])):\r\n",
        "                var7 = params[n[6]][i]\r\n",
        "                for i in range(len(params[n[7]])):\r\n",
        "                  var8 = params[n[7]][i]\r\n",
        "                  for i in range(len(params[n[8]])):\r\n",
        "                    var9 = params[n[8]][i]\r\n",
        "                    for i in range(len(params[n[9]])):\r\n",
        "                      var10 = params[n[9]][i]\r\n",
        "                      lst1.append(var1)\r\n",
        "                      lst2.append(var2)\r\n",
        "                      lst3.append(var3)\r\n",
        "                      lst4.append(var4)\r\n",
        "                      lst5.append(var5)\r\n",
        "                      lst6.append(var6)\r\n",
        "                      lst7.append(var7)\r\n",
        "                      lst8.append(var8)\r\n",
        "                      lst9.append(var9)\r\n",
        "                      lst10.append(var10)\r\n",
        "  df = pd.DataFrame(lst1)\r\n",
        "  df.columns = [n[0]]\r\n",
        "  df[n[1]] = lst2\r\n",
        "  df[n[2]] = lst3\r\n",
        "  df[n[3]] = lst4\r\n",
        "  df[n[4]] = lst5\r\n",
        "  df[n[5]] = lst6\r\n",
        "  df[n[6]] = lst7\r\n",
        "  df[n[7]] = lst8\r\n",
        "  df[n[8]] = lst9\r\n",
        "  df[n[9]] = lst10\r\n",
        "  df['features'] = [n_features] * len(df)\r\n",
        "  df['length'] = [len_dataset] * len(df)\r\n",
        "  df['dominance'] = [dominant_class] * len(df)\r\n",
        "  return df\r\n",
        "                    \r\n",
        "                    \r\n",
        "grid = {'output_dim': [2], #because we have 2 classes\r\n",
        "          'embedding': [45589], #vocab is number of unique words in dataset\r\n",
        "          'nodes': list(range(32, 68, 4)), #we will test between 32 and 64 nodes for the first layer\r\n",
        "          'activation': ['tanh', 'relu'], #we will test between relu and tanh for activation function\r\n",
        "          'regularizer': ['L1', None, 'L2'], #we will use L1 reqularization to prevent overfitting\r\n",
        "          'stacking': [False, True], #stacking makes the first 2 layers the same, we will not do this\r\n",
        "          'dropout': [False, True], #we will not use dropout because we are already using L1 regularization\r\n",
        "          'optimizer': ['adam', 'rmsprop', 'sgd'], #we will test between adam and rmsprop for optimization function\r\n",
        "          'method': ['LSTM', 'GRU'], #we will test between using an LSTM cell and a GRU cell\r\n",
        "          'bidirectional': [True, False]}\r\n",
        "\r\n",
        "NlpGrid = BunnyPinkears(grid, 360, 17, 0.5)    \r\n",
        "NlpGrid"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>output_dim</th>\n",
              "      <th>embedding</th>\n",
              "      <th>nodes</th>\n",
              "      <th>activation</th>\n",
              "      <th>regularizer</th>\n",
              "      <th>stacking</th>\n",
              "      <th>dropout</th>\n",
              "      <th>optimizer</th>\n",
              "      <th>method</th>\n",
              "      <th>bidirectional</th>\n",
              "      <th>features</th>\n",
              "      <th>length</th>\n",
              "      <th>dominance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>tanh</td>\n",
              "      <td>L1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>adam</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>tanh</td>\n",
              "      <td>L1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>adam</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>tanh</td>\n",
              "      <td>L1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>adam</td>\n",
              "      <td>GRU</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>tanh</td>\n",
              "      <td>L1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>adam</td>\n",
              "      <td>GRU</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>32</td>\n",
              "      <td>tanh</td>\n",
              "      <td>L1</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2587</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>L2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>rmsprop</td>\n",
              "      <td>GRU</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2588</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>L2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>sgd</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2589</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>L2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>sgd</td>\n",
              "      <td>LSTM</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2590</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>L2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>sgd</td>\n",
              "      <td>GRU</td>\n",
              "      <td>True</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2591</th>\n",
              "      <td>2</td>\n",
              "      <td>45589</td>\n",
              "      <td>64</td>\n",
              "      <td>relu</td>\n",
              "      <td>L2</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>sgd</td>\n",
              "      <td>GRU</td>\n",
              "      <td>False</td>\n",
              "      <td>17</td>\n",
              "      <td>360</td>\n",
              "      <td>0.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2592 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      output_dim  embedding  nodes  ... features length  dominance\n",
              "0              2      45589     32  ...       17    360        0.5\n",
              "1              2      45589     32  ...       17    360        0.5\n",
              "2              2      45589     32  ...       17    360        0.5\n",
              "3              2      45589     32  ...       17    360        0.5\n",
              "4              2      45589     32  ...       17    360        0.5\n",
              "...          ...        ...    ...  ...      ...    ...        ...\n",
              "2587           2      45589     64  ...       17    360        0.5\n",
              "2588           2      45589     64  ...       17    360        0.5\n",
              "2589           2      45589     64  ...       17    360        0.5\n",
              "2590           2      45589     64  ...       17    360        0.5\n",
              "2591           2      45589     64  ...       17    360        0.5\n",
              "\n",
              "[2592 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    }
  ]
}